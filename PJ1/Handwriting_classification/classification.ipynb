{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The code aims to classify the handwritten words, which is classification in this case.'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "category=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # load the data\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(category):\n",
    "        root = f'../train/{i+1}'\n",
    "        for filename in os.listdir(root):\n",
    "            img_root = os.path.join(root, filename)\n",
    "            image = Image.open(img_root)\n",
    "            x.append(np.array(image).flatten())\n",
    "            y.append([1 if j == i else 0 for j in range(12)])\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    # normalization\n",
    "    x = x/255\n",
    "    return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_MLP(input, output, hidden_layer):\n",
    "    W1 = np.random.randn(input, hidden_layer)\n",
    "    b1 = np.random.randn(hidden_layer)\n",
    "    W2 = np.random.randn(hidden_layer, output)\n",
    "    b2 = np.random.randn(output)\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, W1, b1, W2, b2):\n",
    "    z1 = np.dot(x, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = softmax(z2)\n",
    "    return a2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward(x_, y_, W1, b1, W2, b2, learning_rate):\n",
    "    # x y is the batch data\n",
    "    dW2 = np.zeros_like(W2)\n",
    "    db2 = np.zeros_like(b2)\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    db1 = np.zeros_like(b1)\n",
    "    loss = 0\n",
    "    for i in range(len(x_)):\n",
    "        x = x_[i].reshape(1, -1)\n",
    "        y = y_[i]\n",
    "        z1 = np.dot(x, W1) + b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(a1, W2) + b2\n",
    "        a2 = softmax(z2)\n",
    "        loss += np.sum(-y*np.log(a2))\n",
    "        # calculate the derivative\n",
    "        dW2 += np.dot(a1.T, (-y)/a2*(1-z2)*z2) # (hidden_layer, output), (output, )=>(hidden_layer, output)\n",
    "        db2 += ((-y)/a2*(1-z2)*z2).flatten() # (output, )\n",
    "        dW1 += np.dot(x.T, np.dot(-y/a2*z2*(1-z2), W2.T)*a1*(1-a1)) # (input, hidden_layer), (output, hidden_layer)=>(input, hidden_layer)\n",
    "        db1 += (np.dot(-y/a2*z2*(1-z2), W2.T)*a1*(1-a1)).flatten() # (hidden_layer, )\n",
    "    print(f\"the loss is {loss/len(x_)}\")\n",
    "    W1 -= learning_rate*dW1\n",
    "    b1 -= learning_rate*db1\n",
    "    W2 -= learning_rate*dW2\n",
    "    b2 -= learning_rate*db2\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training(x, y,input,output, hidden_layer,batch_size ,learning_rate, epochs):\n",
    "    W1, b1, W2, b2 = make_MLP(input,output , hidden_layer)\n",
    "    idx = np.random.permutation(len(x))\n",
    "    for i in range(epochs):\n",
    "        print(f\"****the {i}th round epoch****\")\n",
    "        for j in range(0, len(x), batch_size):\n",
    "            x_ = x[idx[j:j+batch_size]]\n",
    "            y_ = y[idx[j:j+batch_size]]\n",
    "            W1, b1, W2, b2 = backward(x_, y_, W1, b1, W2, b2, learning_rate)\n",
    "    \n",
    "    \n",
    "    return [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****the 0th round epoch****\n",
      "the loss is 30.48629485171573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\90707162.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\90707162.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\90707162.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\3378197366.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  loss += np.sum(-y*np.log(a2))\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\3378197366.py:15: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss += np.sum(-y*np.log(a2))\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\3378197366.py:17: RuntimeWarning: divide by zero encountered in divide\n",
      "  dW2 += np.dot(a1.T, (-y)/a2*(1-z2)*z2) # (hidden_layer, output), (output, )=>(hidden_layer, output)\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\3378197366.py:17: RuntimeWarning: invalid value encountered in divide\n",
      "  dW2 += np.dot(a1.T, (-y)/a2*(1-z2)*z2) # (hidden_layer, output), (output, )=>(hidden_layer, output)\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\3378197366.py:18: RuntimeWarning: divide by zero encountered in divide\n",
      "  db2 += ((-y)/a2*(1-z2)*z2).flatten() # (output, )\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\3378197366.py:18: RuntimeWarning: invalid value encountered in divide\n",
      "  db2 += ((-y)/a2*(1-z2)*z2).flatten() # (output, )\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\3378197366.py:19: RuntimeWarning: divide by zero encountered in divide\n",
      "  dW1 += np.dot(x.T, np.dot(-y/a2*z2*(1-z2), W2.T)*a1*(1-a1)) # (input, hidden_layer), (output, hidden_layer)=>(input, hidden_layer)\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\3378197366.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  dW1 += np.dot(x.T, np.dot(-y/a2*z2*(1-z2), W2.T)*a1*(1-a1)) # (input, hidden_layer), (output, hidden_layer)=>(input, hidden_layer)\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\3378197366.py:20: RuntimeWarning: divide by zero encountered in divide\n",
      "  db1 += (np.dot(-y/a2*z2*(1-z2), W2.T)*a1*(1-a1)).flatten() # (hidden_layer, )\n",
      "C:\\Users\\86177\\AppData\\Local\\Temp\\ipykernel_14432\\3378197366.py:20: RuntimeWarning: invalid value encountered in divide\n",
      "  db1 += (np.dot(-y/a2*z2*(1-z2), W2.T)*a1*(1-a1)).flatten() # (hidden_layer, )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss is nan\n",
      "the loss is nan\n",
      "the loss is nan\n",
      "the loss is nan\n",
      "the loss is nan\n",
      "the loss is nan\n",
      "the loss is nan\n",
      "the loss is nan\n",
      "the loss is nan\n",
      "the loss is nan\n",
      "the loss is nan\n",
      "the loss is nan\n",
      "the loss is nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m\n\u001b[0;32m      7\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 8\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 9\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(x, y, input, output, hidden_layer, batch_size, learning_rate, epochs)\u001b[0m\n\u001b[0;32m      7\u001b[0m         x_ \u001b[38;5;241m=\u001b[39m x[idx[j:j\u001b[38;5;241m+\u001b[39mbatch_size]]\n\u001b[0;32m      8\u001b[0m         y_ \u001b[38;5;241m=\u001b[39m y[idx[j:j\u001b[38;5;241m+\u001b[39mbatch_size]]\n\u001b[1;32m----> 9\u001b[0m         W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [W1, b1, W2, b2]\n",
      "Cell \u001b[1;32mIn[26], line 20\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(x_, y_, W1, b1, W2, b2, learning_rate)\u001b[0m\n\u001b[0;32m     18\u001b[0m     db2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m-\u001b[39my)\u001b[38;5;241m/\u001b[39ma2\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mz2)\u001b[38;5;241m*\u001b[39mz2)\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;66;03m# (output, )\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     dW1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x\u001b[38;5;241m.\u001b[39mT, np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;241m-\u001b[39my\u001b[38;5;241m/\u001b[39ma2\u001b[38;5;241m*\u001b[39mz2\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mz2), W2\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m*\u001b[39ma1\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39ma1)) \u001b[38;5;66;03m# (input, hidden_layer), (output, hidden_layer)=>(input, hidden_layer)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     db1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43ma2\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mz2\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mz2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39ma1\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39ma1))\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;66;03m# (hidden_layer, )\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe loss is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(x_)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m W1 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate\u001b[38;5;241m*\u001b[39mdW1\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x, y = load_data()\n",
    "input = x.shape[1]\n",
    "output = y.shape[1]\n",
    "hidden_layer = 1024\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001\n",
    "epochs = 1\n",
    "W1, b1, W2, b2 = training(x, y, input, output, hidden_layer, batch_size, learning_rate, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
